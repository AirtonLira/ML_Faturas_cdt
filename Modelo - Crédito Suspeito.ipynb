{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkContext\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import import_ipynb\n",
    "import utils_cdt as cdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoDataBases = {\"ip\":\"\",\"base\":\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache(self): \n",
    "    \"\"\" \n",
    "    Persist this RDD with the default storage level (C{MEMORY_ONLY_SER}). \n",
    "    \"\"\" \n",
    "    self.is_cached = True \n",
    "    self.persist(StorageLevel.MEMORY_ONLY_SER) \n",
    "    return self "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Modelo - Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Modelo - Spark>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Modelo - Spark\") \\\n",
    "    .config(\"spark.executor.memory\", \"4gb\") \\\n",
    "    .config(\"spark.driver.host\",\"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = cdt.resultQuery(\"creditos_suspeitos.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = cdt.extrairDados(infoDataBases,consulta,\"../data/\",\"creditossuspeitos\",spark=True,objSpark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all distinct Id_Conta\n",
    "df_ids = df.groupBy('Id_Conta').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "#Create an empty dataframe to append de results\n",
    "df_teste = sqlContext.createDataFrame(sc.emptyRDD(), df_daily.schema)\n",
    "df_teste = df_teste.withColumn('ZScore',lit(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate over the id's to calculate the mean and the standard deviation from each account and then calculate its Z-Score\n",
    "#create sql query    \n",
    "consulta_historica = cdt.resultQuery(\"mdl_creditos_suspeitos.sql\")\n",
    "\n",
    "#connect to the database anda extract register 3 months before the transaction from the specified Id_Conta\n",
    "df_historical = cdt.extrairDados(infoDataBases,consulta_historica,spark=True,objSpark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for index, row in df_ids.iterrows():\n",
    "    #df_historical_id = df_historical.where('Id_Conta = '+ str(row['Id_Conta']))\n",
    "\n",
    "    #Get the Mean and Standard Deviation from his historical data\n",
    "    df_stats = df_historical.where('Id_Conta = '+ str(row['Id_Conta'])).select(\n",
    "        _mean(col('Valor')).alias('mean'),\n",
    "        _stddev(col('Valor')).alias('std'),\n",
    "    ).collect()\n",
    "\n",
    "    mean = df_stats[0]['mean']\n",
    "    std = df_stats[0]['std']\n",
    "    \n",
    "    #Calculate the Z-Score of that account\n",
    "    #df_out = df_spark.where('Id_Conta = '+str(row['Id_Conta'])).withColumn('ZScore', (df_spark.Valor - mean)/std)\n",
    "    \n",
    "    #Append to the final DataFrame\n",
    "    df_teste = df_teste.union(df_daily.where('Id_Conta = '+str(row['Id_Conta'])).withColumn('ZScore', (df_daily.Valor - mean)/std))\n",
    "    \n",
    "#Calculate processing time\n",
    "print(str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df_teste.where('Zscore > 3').toPandas()\n",
    "outliers.to_csv('outliersPortadores.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
